#!/bin/bash
#SBATCH --output=outfile-%J
#SBATCH --partition=batch
#SBATCH --nodes=2
#SBATCH --mem-per-cpu=3072
#SBATCH --ntasks-per-node=1
#SBATCH --ntasks=2
#SBATCH --cpus-per-task=16
##SBATCH --time=60:00
#SBATCH --gpus=8
set -eux

echo $SLURM_GPUS
## Date Format
DATESTAMP=${DATESTAMP:-`date +'%y%m%d%H%M%S'`}

## Data, container and volumes
CONT=${CONT:-"gcr.io/data-science-enterprise/spark:3.0.1"}
MOUNT=${MOUNT:-"/data"} 
LOGDIR=${LOGDIR:-"/data/log"}

sudo mkdir -p ${LOGDIR}
sudo chown -R $(id -u):$(id -g) ${LOGDIR}

# Other vars
#readonly _logfile_base="${LOGDIR}/${DATESTAMP}"
#readonly _cont_name=spark
#readonly _cont_mounts="${MOUNT}:/opt/
_logfile_base="${LOGDIR}/${DATESTAMP}"
_cont_name="spark_${SLURM_JOB_ID}"
_cont_mounts="${MOUNT}/spark/conf:/opt/spark/conf,${MOUNT}/spark/log:/opt/spark/log"

## Docker params
##export VOLS="-v $DATADIR:/data -v $LOGDIR:/results"
##export CONTNAME="${SLURM_JOB_ID}"

##MASTER_IP=`getent hosts \`hostname\` | cut -d ' ' -f1`
##export hosts=( `scontrol show hostname |tr "\n" " "` )
##unique_hosts=( $(echo "${hosts[@]}" | tr ' ' '\n' | sort -u | tr '\n' ' ' ) )
##export MASTER_HOST=${hosts[0]}

## Prepull container to shared storage
#mkdir -p ${MOUNT}/containers |& tee "${_logfile_base}_${SLURM_JOBID}.log"
#CONT_FILE="${MOUNT}/containers/${SLURM_JOBID}_$(basename ${CONT}).squashfs"

##NR_PROCS=$(($SLURM_NTASKS))
##for PROC in $(seq 0 $(($NR_PROCS-1)));
##do
    #My call looks like this:
##    srun --exclusive -N1 -n1 --container-imag gcr.io#data-science-enterprise/spark:3.0.1 --container-save ${CONT_FILE} true &
##    srun --exclusive -N1 -n1 hostname &
##    pids[${PROC}]=$!    #Save PID of this background process
##done
##3for pid in ${pids[*]};
##do
##    wait ${pid} #Wait on all PIDs, this returns 0 if ANY process fails
##done

##srun --exclusive --nodes=1 --ntasks=1 enroot import --output ${CONT_FILE} dockerd://${CONT} |& tee "${_logfile_base}_${SLURM_JOBID}.log"
##srun --ntasks=1 --container-image nvcr.io#nvidia/pytorch:20.03-py3 --container-save /lustre/felix/pytorch.sqsh true
#srun --ntasks=1 --container-imag gcr.io#data-science-enterprise/spark:3.0.1 --container-save ${CONT_FILE} true
##srun --ntasks=1 --nodes='1' enroot import --output ${CONT_FILE} dockerd://${CONT} |& tee "${_logfile_base}_${SLURM_JOBID}.log"

## Set concurrent GPU's meaning the amount threads per GPU
MASTER=`hostname`
echo "$MASTER"
CONCURRENTGPU='4'
GPU_PER_NODE=$(( ${SLURM_GPUS} / ${SLURM_JOB_NUM_NODES} ))
TOTAL_CORES=$(( ${SLURM_CPUS_PER_TASK} * ${SLURM_NTASKS} ))
NUM_EXECUTORS=$SLURM_GPUS
NUM_EXECUTOR_CORES=$(( ${TOTAL_CORES} / ${NUM_EXECUTORS} ))
SPARK_WORKER_MEMORY=$(( $TOTAL_CORES * $SLURM_MEM_PER_CPU ))M

RESOURCE_GPU_AMT=$(echo "scale=3; ${NUM_EXECUTORS} / ${TOTAL_CORES}" | bc)
SPARK_HOME=$MOUNT/spark
SPARK_RAPIDS_DIR=$MOUNT/sparkRapidsPlugin
WORKER_OPTS="-Dspark.worker.resource.gpu.amount=$GPU_PER_NODE -Dspark.worker.resource.gpu.discoveryScript=$SPARK_RAPIDS_DIR/getGpusResources.sh"

srun -pdebug --ntasks="${SLURM_JOB_NUM_NODES}" bash -c "echo -n 'Clearing cache on ' && hostname && sync && sudo /sbin/sysctl vm.drop_caches=3" | tee "${_logfile_base}_${SLURM_JOBID}.log"

env=$SPARK_HOME/conf/spark-env.sh
##echo "#! /bin/bash" > $env
echo "export SPARK_LOG_DIR=$SPARK_HOME/log" > $env
echo "export SPARK_WORKER_DIR=$SPARK_HOME/sparkworker" >> $env
echo "export SLURM_MEM_PER_CPU=$SLURM_MEM_PER_CPU" >> $env
echo 'export SPARK_WORKER_CORES=`nproc`' >> $env
echo "export SPARK_WORKER_MEMORY=$SPARK_WORKER_MEMORY" >> $env
echo "export SPARK_WORKER_OPTS='"$WORKER_OPTS"'" >> $env

sudo chmod 777 $env

scontrol show hostname $SLURM_JOB_NODELIST > $SPARK_HOME/conf/slaves

conf=$SPARK_HOME/conf/spark-defaults.conf
echo "spark.default.parallelism" $(( $SLURM_CPUS_PER_TASK * $SLURM_NTASKS ))> $conf
echo "spark.submit.deployMode" client >> $conf
echo "spark.master" spark://`hostname`:7077 >> $conf
echo "spark.executor.cores" $NUM_EXECUTOR_CORES >> $conf
echo "spark.executor.memory" $((( $SLURM_CPUS_PER_TASK * $SLURM_MEM_PER_CPU / $GPU_PER_NODE )))M >> $conf

##export DOCKEREXEC="nvidia-docker run --rm --net=host --uts=host --ipc=host --ulimit stack=67108864 --ulimit memlock=-1"
##srun -pdebug -n $SLURM_JOB_NUM_NODES --ntasks-per-node=1 $DOCKEREXEC --name $CONTNAME $VOLS $VARS $CONT bash -c 'sleep infinity'
srun -pdebug -n $SLURM_JOB_NUM_NODES --ntasks-per-node=1 docker run --runtime=nvidia -v /data/spark/conf:/opt/spark/conf,/data/spark/log:/opt/spark/log --network host -it gcr.io/data-science-enterprise/spark:3.0.1 --entrypoint $SPARK_HOME/bin/spark-class org.apache.spark.deploy.master.Master

echo "*****STOP*****"
###srun -pdebug --ntasks="$(( SLURM_JOB_NUM_NODES))" --container-image="${CONT_FILE}" --container-name="${_cont_name}" true | tee "${_logfile_base}_${SLURM_JOBID}.log"
srun -pdebug --ntasks="$(( SLURM_JOB_NUM_NODES))" --container-imag "gcr.io#data-science-enterprise/spark:3.0.1" --container-name="${_cont_name}" --container-mounts="${_cont_mounts}" true | tee "${_logfile_base}_${SLURM_JOBID}.log"
##srun --ntasks=1 --container-imag gcr.io#data-science-enterprise/spark:3.0.1

##srun  -n $SLURM_JOB_NUM_NODES --ntasks-per-node=1 $DOCKEREXEC --name $CONTNAME $VOLS $CONT bash -c 'sleep infinity'

#srun -l --ntasks="1" --nodes="1" --container-name="${_cont_name}" --container-mounts="${_cont_mounts}" sh -c "/opt/spark/sbin/start-master.sh" |& tee "${_logfile_base}_${SLURM_JOBID}.log"
srun -pdebug -l --container-name="${_cont_name}" --container-mounts="${_cont_mounts}" | tee "${_logfile_base}_${SLURM_JOBID}.log" 

srun -pdebug -l -w `hostname` --exclusive --container-name="${_cont_name}" --container-mounts="${_cont_mounts}" /opt/spark/sbin/start-master.sh bash -c 'sleep infinity'  |& tee "${_logfile_base}_${SLURM_JOBID}.log"

srun -pdebug -l  --exclusive --container-name="${_cont_name}" --container-mounts="${_cont_mounts}" /opt/spark/sbin/start-slave.sh $MASTER |& tee "${_logfile_base}_${SLURM_JOBID}.log"

. wait-worker.sh 

echo "****Thank you, NEXT!****"

sleep infinity
